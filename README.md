<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css" integrity="sha384-KiWOvVjnN8qwAZbuQyWDIbfCLFhLXNETzBQjA/92pIowpC0d2O3nppDGQVgwd2nB" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js" integrity="sha384-0fdwu/T/EQMsQlrHCCHoH10pkPLlKA1jL5dFyUOvB3lfeT2540/2g6YgSi2BL14p" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/contrib/auto-render.min.js" integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR" crossorigin="anonymous"></script>
<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
          // customised options
          // • auto-render specific keys, e.g.:
          delimiters: [
              {left: '$$', right: '$$', display: true},
              {left: '$', right: '$', display: false},
              {left: '\\(', right: '\\)', display: false},
              {left: '\\[', right: '\\]', display: true}
          ],
          // • rendering keys, e.g.:
          throwOnError : false
        });
    });
</script>

# Introduction

In real world application cases it is very difficult to have clean data set without noises., for this reason it is essential to have an approach to be able to estimate the uncertainty of the classification. one of the solution may be considered Bayesian Convolutional Neural Networks that allow to find posterior distribution from which can be found uncertainty of the model parameters. However it is computationally intractable since require to perform integration over entire the space of parameters of the model.[Kwon et al., 2018] The solution of this problem is the drop out method that allow to approximate the Bayesian inference. [Gal, 2015]

## set up and load data

In order to load train and test data sets I have implement the function "load_data", that in place convert all labels in one hot encoding. In the case of train data set all data is splitted into train and validation subsets by sklearn function "train_test_split" in proportion 80/20. Also for convenience of noise analysis i convert data set of real class labels into a dictionary with string of real class as keys and as values the array of indexes of the corresponding data set vector.

# model

Model is generated by a function "generate_model", that take as arguments the number of classes, input size, number of layers and dropout rate. Such method allows me to have better control over some initial set up of the model and architecture. So after several attempt I was able to find optimal configuration for the model.

In order to implement Bayesian Convolutional Neural Network for multi class classification I choose the sequential architecture composed of 3 convolutional layers each of that is followed by max pooling layer and dropout layer. As final stage I added the flatten layer that prepare the input for dense layer followed again by drop out layer. At each new convolutional layer I double the number of filters - starting from 32 - and set the kernel dimension to (3,3), except the first one where I used kernel dimension (5,5). For each layer I used "same" padding and the relu-activation function, as consequence I used he_normal-initializer for weight initialization. Instead for the last output dense layer I used softmax-activation function as have been suggested by [Kwon et al., 2018]. also I forced drop out layer to be active in training and test phase. The main role of the dropout is to randomly turn off some of the inputs to the layer. In this way in training phase it prevent oferfitting, indeed in this way neurons doesn't learn exactly the training data set. From other hand in test phase drop out allow some degree of uncertainty in evaluation and prediction of new samples that allow to approximate posterior distribution[Gal, 2015] and thus estimate variations.

I compile the model with categorical cross entropy as a loss function, Adam optimizer and accuracy as metrics. After that I trained the model for 25 epoch, that seems optimal in training and evaluation performance, since training and evaluation loss/accuracy started to deviate.

Finally I saved the model and history.

## performance

As may be seen from plot below accuracy and the loss of the train and validation data set present the same behavior, except some fluctuation in validation accuracy and loss. Also both characteristics achieve almost stable and satisfactory values of accuracy and loss. In this way at first analysis the model learn well from training data set and avoid overfitting.

## evaluate model on test data set

At the next step I evaluated model on the trained data set. As results obtained values of loss is quite hight and accuracy is rather low. This can be explained by the fact that in train and test data present noise that give low contribution to overall accuracy and hight contribution to the loss.

# uncertainty analisis

Since the noise data is labeled in the same way as the samples of interests, it is required an approach to to deal with such miss classifications. One way may be to evaluate the uncertainty of all test sample and set a threshold that would cut off most of the noise samples.

## epistemic uncertainty

Using the method proposed in [Kwon et al., 2018] it is possible to estimate aleatoric and epistemic uncertainty. Aleatoric uncertainty capturing noise inherent in the observations and epistemic uncertainty accounts for model uncertainty, so it may be used to evaluate the uncertainty on the prediction done by the model. Indeed due to implementing drop out in the evaluation phase prediction probability $\hat{p}_t = p(\omega_t) = SoftMax(f^{\hat{\omega}_t}(x*))$ will be different in each evaluation experiment and effect of drop out may be seen as randomization of parameters of the model $\omega$ according to the variational predictive distribution $q_{\hat{\theta}}(\omega)$[Kwon et al., 2018]

In ordder to calculate the epistemic uncertainty I used formula 4) from [Kwon et al., 2018]:

$$
\begin{align*}
Var(y) &= \frac{1}{T} \sum_{t=1}^T (\hat{p}_t - \bar{p})(\hat{p}_t - \bar{p})^T\\
\bar{p} &= \frac{1}{T} \sum_{t=1}^T \hat{p}\\
\hat{p}_t &= p(\omega_t) = SoftMax(f^{\hat{\omega}_t}(x*))
\end{align*}
$$

,where $f^{\hat{\omega}_t}(x*)$ is the output of the last dense layer.

the implementation of calculus of epistemic uncertainty have been done in the function "get_epistemic_uncertainty", that have been adapted from the code proposed by [Kwon et al., 2018]. Using the mean prediction probability the samples are classified considering label with probability larger than 0.5 - unambiguous classification - the other samples are rejected.

In order to find the distribution of the uncertainty over all samples I run the loop over entire test set and calculated it for each sample. After that I sorted unambiguously classified samples according to the real labels into corresponding arrays. In this way the noise analysis can be done more effectively and can be avoid the problem of false positive. from the other hand to estimate miss classification of the labels of interest (e.i. "AC", "AD", "H") I store classified labels into dictionary with corresponding real class as keys.

## prediction rate

From the table below can be seen the number of unambiguously classified sample for each label. Neural Network is able pretty well to recognize classes of interest.

From the other hand noise classes are recognized very bad and most of them are rejected: for example NN wasn't able to discriminate none of samples of glass-class. The rest of noise classes still present very low classification rate. This is desired result since we are are interested into reducing the noise into labeling of the samples of interest.

## threshold estimation  

At the next step I plotted the density of recognized samples vs corresponding uncertainty. The number of samples have been normalized to the number of recognized samples, in this way the density of different classes can be compared more effectively. from the plot below can be seen the the density of valid samples is highly picked around zero uncertainty.

Instead for the noise samples distribution may be seen that most of the samples have large enough uncertainty and most of them may be cut be threshold. However some of the noise classes present a small pick at zero uncertainty, even if very small w.r.t the valid samples density. Due to this fact some fraction of noise persist among right recognized class of interest. However considering that it's only a fraction of already reduced data set overall number of miss classification labels should be rather low.
